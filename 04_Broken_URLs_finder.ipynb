{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Broken URLs finder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGtahRS8vnqmR/cprgF76m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcoRocchi/Data-science-with-Python/blob/main/04_Broken_URLs_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz-cV1nqQHDP"
      },
      "source": [
        "#Broken URL finder\n",
        "Given a valid URL, the program returns a list of broken URLs on the selected page.  \n",
        "This time we are going to use specific libraries to manipulate HTML files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4h0gaenRJcf"
      },
      "source": [
        "Let's start by importing a few useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzbmiZluPow2"
      },
      "source": [
        "!pip install validators\n",
        "\n",
        "import validators\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YOUz9_mRz9m"
      },
      "source": [
        "As in a [previous example](https://github.com/MarcoRocchi/Data-science-with-Python/blob/main/01_Words_counter.ipynb), we start asking the user to input an URL and we check the validity of the given URL. If the URL is valid we create an instance of BeautifulSoup passing as data to analize the selected page.  We choosed html5lib as HTML paresr because it's capability to analize complex HTML pages. If the pages you need to analize are \"simply\" you can use the faster \"html.parser\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svanjyroSAuB"
      },
      "source": [
        "page_url = input(\"Enter the page URL \")\n",
        "if validators.url(page_url) == False:\n",
        "  print(\"The given string is not a well formed URL, execution stops\")\n",
        "else:\n",
        "  try:\n",
        "    soup = BeautifulSoup(urlopen(page_url), \"html5lib\")\n",
        "  except:\n",
        "    print(\"An error occurred while opening the given URL, please check if it's correct and try again\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkCFxBzVW9h"
      },
      "source": [
        "Let's now extract a list of all the links using the found function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LffJre6ZVQ-c"
      },
      "source": [
        "links = [link[\"href\"]\n",
        "for link in soup.find_all(\"a\")\n",
        "if link.has_attr(\"href\")]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QTMZ_kaWXjH"
      },
      "source": [
        "When the urlopen function does not throw an exception, we have a valid URL, otherwise we have a broken URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZftjJjSWfAT"
      },
      "source": [
        "for link in links:\n",
        "  try:  \n",
        "    urlopen(link)\n",
        "  except:\n",
        "    print(link) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}